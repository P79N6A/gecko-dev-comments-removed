





















import errno
import sys
import platform
import os
import re
import shutil
import textwrap
import fnmatch
import subprocess
import urlparse
import multiprocessing
import collections
from optparse import OptionParser
from xml.dom.minidom import parse

from mozpack.copier import FileRegistry
from mozpack.manifests import (
    InstallManifest,
    UnreadableInstallManifest,
)



class VCSFileInfo:
    """ A base class for version-controlled file information. Ensures that the
        following attributes are generated only once (successfully):

            self.root
            self.clean_root
            self.revision
            self.filename

        The attributes are generated by a single call to the GetRoot,
        GetRevision, and GetFilename methods. Those methods are explicitly not
        implemented here and must be implemented in derived classes. """

    def __init__(self, file):
        if not file:
            raise ValueError
        self.file = file

    def __getattr__(self, name):
        """ __getattr__ is only called for attributes that are not set on self,
            so setting self.[attr] will prevent future calls to the GetRoot,
            GetRevision, and GetFilename methods. We don't set the values on
            failure on the off chance that a future call might succeed. """

        if name == "root":
            root = self.GetRoot()
            if root:
                self.root = root
            return root

        elif name == "clean_root":
            clean_root = self.GetCleanRoot()
            if clean_root:
                self.clean_root = clean_root
            return clean_root

        elif name == "revision":
            revision = self.GetRevision()
            if revision:
                self.revision = revision
            return revision

        elif name == "filename":
            filename = self.GetFilename()
            if filename:
                self.filename = filename
            return filename

        raise AttributeError

    def GetRoot(self):
        """ This method should return the unmodified root for the file or 'None'
            on failure. """
        raise NotImplementedError

    def GetCleanRoot(self):
        """ This method should return the repository root for the file or 'None'
            on failure. """
        raise NotImplementedErrors

    def GetRevision(self):
        """ This method should return the revision number for the file or 'None'
            on failure. """
        raise NotImplementedError

    def GetFilename(self):
        """ This method should return the repository-specific filename for the
            file or 'None' on failure. """
        raise NotImplementedError










rootRegex = re.compile(r'^\S+?:/+(?:[^\s/]*@)?(\S+)$')

def read_output(*args):
    (stdout, _) = subprocess.Popen(args=args, stdout=subprocess.PIPE).communicate()
    return stdout.rstrip()

class HGRepoInfo:
    def __init__(self, path):
        self.path = path
        rev = read_output('hg', '-R', path,
                          'parent', '--template={node|short}')
        
        
        hg_root = os.environ.get("SRCSRV_ROOT")
        if hg_root:
            root = hg_root
        else:
            root = read_output('hg', '-R', path,
                               'showconfig', 'paths.default')
            if not root:
                print >> sys.stderr, "Failed to get HG Repo for %s" % path
        cleanroot = None
        if root:
            match = rootRegex.match(root)
            if match:
                cleanroot = match.group(1)
                if cleanroot.endswith('/'):
                    cleanroot = cleanroot[:-1]
        if cleanroot is None:
            print >> sys.stderr, textwrap.dedent("""\
                Could not determine repo info for %s.  This is either not a clone of the web-based
                repository, or you have not specified SRCSRV_ROOT, or the clone is corrupt.""") % path
            sys.exit(1)
        self.rev = rev
        self.root = root
        self.cleanroot = cleanroot

    def GetFileInfo(self, file):
        return HGFileInfo(file, self)

class HGFileInfo(VCSFileInfo):
    def __init__(self, file, repo):
        VCSFileInfo.__init__(self, file)
        self.repo = repo
        self.file = os.path.relpath(file, repo.path)

    def GetRoot(self):
        return self.repo.root

    def GetCleanRoot(self):
        return self.repo.cleanroot

    def GetRevision(self):
        return self.repo.rev

    def GetFilename(self):
        if self.revision and self.clean_root:
            return "hg:%s:%s:%s" % (self.clean_root, self.file, self.revision)
        return self.file

class GitRepoInfo:
    """
    Info about a local git repository. Does not currently
    support discovering info about a git clone, the info must be
    provided out-of-band.
    """
    def __init__(self, path, rev, root):
        self.path = path
        cleanroot = None
        if root:
            match = rootRegex.match(root)
            if match:
                cleanroot = match.group(1)
                if cleanroot.endswith('/'):
                    cleanroot = cleanroot[:-1]
        if cleanroot is None:
            print >> sys.stderr, textwrap.dedent("""\
                Could not determine repo info for %s (%s).  This is either not a clone of a web-based
                repository, or you have not specified SRCSRV_ROOT, or the clone is corrupt.""") % (path, root)
            sys.exit(1)
        self.rev = rev
        self.cleanroot = cleanroot

    def GetFileInfo(self, file):
        return GitFileInfo(file, self)

class GitFileInfo(VCSFileInfo):
    def __init__(self, file, repo):
        VCSFileInfo.__init__(self, file)
        self.repo = repo
        self.file = os.path.relpath(file, repo.path)

    def GetRoot(self):
        return self.repo.path

    def GetCleanRoot(self):
        return self.repo.cleanroot

    def GetRevision(self):
        return self.repo.rev

    def GetFilename(self):
        if self.revision and self.clean_root:
            return "git:%s:%s:%s" % (self.clean_root, self.file, self.revision)
        return self.file





vcsFileInfoCache = {}

def IsInDir(file, dir):
    
    
    
    return os.path.abspath(file).lower().startswith(os.path.abspath(dir).lower())

def GetVCSFilenameFromSrcdir(file, srcdir):
    if srcdir not in Dumper.srcdirRepoInfo:
        
        if os.path.isdir(os.path.join(srcdir, '.hg')):
            Dumper.srcdirRepoInfo[srcdir] = HGRepoInfo(srcdir)
        else:
            
            return None
    return Dumper.srcdirRepoInfo[srcdir].GetFileInfo(file)

def GetVCSFilename(file, srcdirs):
    """Given a full path to a file, and the top source directory,
    look for version control information about this file, and return
    a tuple containing
    1) a specially formatted filename that contains the VCS type,
    VCS location, relative filename, and revision number, formatted like:
    vcs:vcs location:filename:revision
    For example:
    cvs:cvs.mozilla.org/cvsroot:mozilla/browser/app/nsBrowserApp.cpp:1.36
    2) the unmodified root information if it exists"""
    (path, filename) = os.path.split(file)
    if path == '' or filename == '':
        return (file, None)

    fileInfo = None
    root = ''
    if file in vcsFileInfoCache:
        
        fileInfo = vcsFileInfoCache[file]
    else:
        for srcdir in srcdirs:
            if not IsInDir(file, srcdir):
                continue
            fileInfo = GetVCSFilenameFromSrcdir(file, srcdir)
            if fileInfo:
                vcsFileInfoCache[file] = fileInfo
                break

    if fileInfo:
        file = fileInfo.filename
        root = fileInfo.root

    
    return (file.replace("\\", "/"), root)

def validate_install_manifests(install_manifest_args):
    args = []
    for arg in install_manifest_args:
        bits = arg.split(',')
        if len(bits) != 2:
            raise ValueError('Invalid format for --install-manifest: '
                             'specify manifest,target_dir')
        manifest_file, destination = map(os.path.abspath, bits)
        if not os.path.isfile(manifest_file):
            raise IOError(errno.ENOENT, 'Manifest file not found',
                          manifest_file)
        if not os.path.isdir(destination):
            raise IOError(errno.ENOENT, 'Install directory not found',
                          destination)
        try:
            manifest = InstallManifest(manifest_file)
        except UnreadableInstallManifest:
            raise IOError(errno.EINVAL, 'Error parsing manifest file',
                          manifest_file)
        args.append((manifest, destination))
    return args

def make_file_mapping(install_manifests):
    file_mapping = {}
    for manifest, destination in install_manifests:
        destination = os.path.abspath(destination)
        reg = FileRegistry()
        manifest.populate_registry(reg)
        for dst, src in reg:
            if hasattr(src, 'path'):
                abs_dest = os.path.normpath(os.path.join(destination, dst))
                file_mapping[abs_dest] = src.path
    return file_mapping

def GetPlatformSpecificDumper(**kwargs):
    """This function simply returns a instance of a subclass of Dumper
    that is appropriate for the current platform."""
    
    
    return {'Windows': Dumper_Win32,
            'Microsoft': Dumper_Win32,
            'Linux': Dumper_Linux,
            'Sunos5': Dumper_Solaris,
            'Darwin': Dumper_Mac}[platform.system()](**kwargs)

def SourceIndex(fileStream, outputPath, vcs_root):
    """Takes a list of files, writes info to a data block in a .stream file"""
    
    
    result = True
    pdbStreamFile = open(outputPath, "w")
    pdbStreamFile.write('''SRCSRV: ini ------------------------------------------------\r\nVERSION=2\r\nINDEXVERSION=2\r\nVERCTRL=http\r\nSRCSRV: variables ------------------------------------------\r\nHGSERVER=''')
    pdbStreamFile.write(vcs_root)
    pdbStreamFile.write('''\r\nSRCSRVVERCTRL=http\r\nHTTP_EXTRACT_TARGET=%hgserver%/raw-file/%var3%/%var2%\r\nSRCSRVTRG=%http_extract_target%\r\nSRCSRV: source files ---------------------------------------\r\n''')
    pdbStreamFile.write(fileStream) 
    pdbStreamFile.write("SRCSRV: end ------------------------------------------------\r\n\n")
    pdbStreamFile.close()
    return result

def WorkerInitializer(cls, lock, srcdirRepoInfo):
    """Windows worker processes won't have run GlobalInit, and due to a lack of fork(),
    won't inherit the class variables from the parent. They only need a few variables,
    so we run an initializer to set them. Redundant but harmless on other platforms."""
    cls.lock = lock
    cls.srcdirRepoInfo = srcdirRepoInfo

def StartProcessFilesWork(dumper, files, arch_num, arch, vcs_root, after, after_arg):
    """multiprocessing can't handle methods as Process targets, so we define
    a simple wrapper function around the work method."""
    return dumper.ProcessFilesWork(files, arch_num, arch, vcs_root, after, after_arg)

class Dumper:
    """This class can dump symbols from a file with debug info, and
    store the output in a directory structure that is valid for use as
    a Breakpad symbol server.  Requires a path to a dump_syms binary--
    |dump_syms| and a directory to store symbols in--|symbol_path|.
    Optionally takes a list of processor architectures to process from
    each debug file--|archs|, the full path to the top source
    directory--|srcdir|, for generating relative source file names,
    and an option to copy debug info files alongside the dumped
    symbol files--|copy_debug|, mostly useful for creating a
    Microsoft Symbol Server from the resulting output.

    You don't want to use this directly if you intend to call
    ProcessDir.  Instead, call GetPlatformSpecificDumper to
    get an instance of a subclass.
 
    Processing is performed asynchronously via worker processes; in
    order to wait for processing to finish and cleanup correctly, you
    must call Finish after all Process/ProcessDir calls have been made.
    You must also call Dumper.GlobalInit before creating or using any
    instances."""
    def __init__(self, dump_syms, symbol_path,
                 archs=None,
                 srcdirs=[],
                 copy_debug=False,
                 vcsinfo=False,
                 srcsrv=False,
                 exclude=[],
                 repo_manifest=None,
                 file_mapping=None):
        
        self.dump_syms = os.path.abspath(dump_syms)
        self.symbol_path = symbol_path
        if archs is None:
            
            self.archs = ['']
        else:
            self.archs = ['-a %s' % a for a in archs.split()]
        self.srcdirs = [os.path.normpath(a) for a in srcdirs]
        self.copy_debug = copy_debug
        self.vcsinfo = vcsinfo
        self.srcsrv = srcsrv
        self.exclude = exclude[:]
        if repo_manifest:
            self.parse_repo_manifest(repo_manifest)
        self.file_mapping = file_mapping or {}

        
        self.files_record = {}
        self.jobs_record = collections.defaultdict(int)

    @classmethod
    def GlobalInit(cls, module=multiprocessing):
        """Initialize the class globals for the multiprocessing setup; must
        be called before any Dumper instances are created and used. Test cases
        may pass in a different module to supply Manager and Pool objects,
        usually multiprocessing.dummy."""
        num_cpus = module.cpu_count()
        if num_cpus is None:
            
            
            num_cpus = 2

        
        cls.manager = module.Manager()
        cls.jobs_condition = Dumper.manager.Condition()
        cls.lock = Dumper.manager.RLock()
        cls.srcdirRepoInfo = Dumper.manager.dict()
        cls.pool = module.Pool(num_cpus, WorkerInitializer,
                               (cls, cls.lock, cls.srcdirRepoInfo))

    def JobStarted(self, file_key):
        """Increments the number of submitted jobs for the specified key file,
        defined as the original file we processed; note that a single key file
        can generate up to 1 + len(self.archs) jobs in the Mac case."""
        with Dumper.jobs_condition:
            self.jobs_record[file_key] += 1
            Dumper.jobs_condition.notify_all()

    def JobFinished(self, file_key):
        """Decrements the number of submitted jobs for the specified key file,
        defined as the original file we processed; once the count is back to 0,
        remove the entry from our record."""
        with Dumper.jobs_condition:
            self.jobs_record[file_key] -= 1

            if self.jobs_record[file_key] == 0:
                del self.jobs_record[file_key]

            Dumper.jobs_condition.notify_all()

    def output(self, dest, output_str):
        """Writes |output_str| to |dest|, holding |lock|;
        terminates with a newline."""
        with Dumper.lock:
            dest.write(output_str + "\n")
            dest.flush()

    def output_pid(self, dest, output_str):
        """Debugging output; prepends the pid to the string."""
        self.output(dest, "%d: %s" % (os.getpid(), output_str))

    def parse_repo_manifest(self, repo_manifest):
        """
        Parse an XML manifest of repository info as produced
        by the `repo manifest -r` command.
        """
        doc = parse(repo_manifest)
        if doc.firstChild.tagName != "manifest":
            return
        
        def ensure_slash(u):
            if not u.endswith("/"):
                return u + "/"
            return u
        remotes = dict([(r.getAttribute("name"), ensure_slash(r.getAttribute("fetch"))) for r in doc.getElementsByTagName("remote")])
        
        default_remote = None
        if doc.getElementsByTagName("default"):
            default_remote = doc.getElementsByTagName("default")[0].getAttribute("remote")
        
        base_dir = os.path.abspath(os.path.dirname(repo_manifest))
        for proj in doc.getElementsByTagName("project"):
            
            name = proj.getAttribute("name")
            
            path = proj.getAttribute("path")
            
            rev = proj.getAttribute("revision")
            
            remote = proj.getAttribute("remote")
            
            if not remote:
                remote = default_remote
            
            if not path:
                path = name
            if not (name and path and rev and remote):
                print "Skipping project %s" % proj.toxml()
                continue
            remote = remotes[remote]
            
            if remote.startswith("git:"):
                remote = "http" + remote[3:]
            
            srcdir = os.path.join(base_dir, path)
            self.srcdirs.append(srcdir)
            
            
            root = urlparse.urljoin(remote, name)
            Dumper.srcdirRepoInfo[srcdir] = GitRepoInfo(srcdir, rev, root)

    
    def ShouldProcess(self, file):
        return not any(fnmatch.fnmatch(os.path.basename(file), exclude) for exclude in self.exclude)

    
    def ShouldSkipDir(self, dir):
        return False

    def RunFileCommand(self, file):
        """Utility function, returns the output of file(1)"""
        try:
            
            
            return os.popen("file -Lb " + file).read()
        except:
            return ""

    
    def FixFilenameCase(self, file):
        return file

    
    def SourceServerIndexing(self, debug_file, guid, sourceFileStream, vcs_root):
        return ""

    
    def CopyDebug(self, file, debug_file, guid):
        pass

    def Finish(self, stop_pool=True):
        """Wait for the expected number of jobs to be submitted, and then
        wait for the pool to finish processing them. By default, will close
        and clear the pool, but for testcases that need multiple runs, pass
        stop_pool = False."""
        with Dumper.jobs_condition:
            while len(self.jobs_record) != 0:
                Dumper.jobs_condition.wait()
        if stop_pool:
            Dumper.pool.close()
            Dumper.pool.join()

    def Process(self, file_or_dir):
        """Process a file or all the (valid) files in a directory; processing is performed
        asynchronously, and Finish must be called to wait for it complete and cleanup."""
        if os.path.isdir(file_or_dir) and not self.ShouldSkipDir(file_or_dir):
            self.ProcessDir(file_or_dir)
        elif os.path.isfile(file_or_dir):
            self.ProcessFiles((file_or_dir,))

    def ProcessDir(self, dir):
        """Process all the valid files in this directory.  Valid files
        are determined by calling ShouldProcess; processing is performed
        asynchronously, and Finish must be called to wait for it complete and cleanup."""
        for root, dirs, files in os.walk(dir):
            for d in dirs[:]:
                if self.ShouldSkipDir(d):
                    dirs.remove(d)
            for f in files:
                fullpath = os.path.join(root, f)
                if self.ShouldProcess(fullpath):
                    self.ProcessFiles((fullpath,))

    def SubmitJob(self, file_key, func, args, callback):
        """Submits a job to the pool of workers; increments the number of submitted jobs."""
        self.JobStarted(file_key)
        res = Dumper.pool.apply_async(func, args=args, callback=callback)

    def ProcessFilesFinished(self, res):
        """Callback from multiprocesing when ProcessFilesWork finishes;
        run the cleanup work, if any"""
        self.JobFinished(res['files'][-1])
        
        self.files_record[res['files']] += 1
        if self.files_record[res['files']] == len(self.archs):
            del self.files_record[res['files']]
            if res['after']:
                res['after'](res['status'], res['after_arg'])

    def ProcessFiles(self, files, after=None, after_arg=None):
        """Dump symbols from these files into a symbol file, stored
        in the proper directory structure in  |symbol_path|; processing is performed
        asynchronously, and Finish must be called to wait for it complete and cleanup.
        All files after the first are fallbacks in case the first file does not process
        successfully; if it does, no other files will be touched."""
        self.output_pid(sys.stderr, "Submitting jobs for files: %s" % str(files))

        
        
        vcs_root = os.environ.get("SRCSRV_ROOT")
        for arch_num, arch in enumerate(self.archs):
            self.files_record[files] = 0 
            self.SubmitJob(files[-1], StartProcessFilesWork, args=(self, files, arch_num, arch, vcs_root, after, after_arg), callback=self.ProcessFilesFinished)

    def ProcessFilesWork(self, files, arch_num, arch, vcs_root, after, after_arg):
        self.output_pid(sys.stderr, "Worker processing files: %s" % (files,))

        
        result = { 'status' : False, 'after' : after, 'after_arg' : after_arg, 'files' : files }

        sourceFileStream = ''
        for file in files:
            
            try:
                proc = subprocess.Popen([self.dump_syms] + arch.split() + [file],
                                        stdout=subprocess.PIPE)
                module_line = proc.stdout.next()
                if module_line.startswith("MODULE"):
                    
                    (guid, debug_file) = (module_line.split())[3:5]
                    
                    sym_file = re.sub("\.pdb$", "", debug_file) + ".sym"
                    
                    rel_path = os.path.join(debug_file,
                                            guid,
                                            sym_file).replace("\\", "/")
                    full_path = os.path.normpath(os.path.join(self.symbol_path,
                                                              rel_path))
                    try:
                        os.makedirs(os.path.dirname(full_path))
                    except OSError: 
                        pass
                    f = open(full_path, "w")
                    f.write(module_line)
                    
                    for line in proc.stdout:
                        if line.startswith("FILE"):
                            
                            (x, index, filename) = line.rstrip().split(None, 2)
                            filename = os.path.normpath(self.FixFilenameCase(filename))
                            if filename in self.file_mapping:
                                filename = self.file_mapping[filename]
                            sourcepath = filename
                            if self.vcsinfo:
                                (filename, rootname) = GetVCSFilename(filename, self.srcdirs)
                                
                                if vcs_root is None:
                                  if rootname:
                                     vcs_root = rootname
                            
                            if filename.startswith("hg"):
                                (ver, checkout, source_file, revision) = filename.split(":", 3)
                                sourceFileStream += sourcepath + "*" + source_file + '*' + revision + "\r\n"
                            f.write("FILE %s %s\n" % (index, filename))
                        else:
                            
                            f.write(line)
                            
                            result['status'] = True
                    f.close()
                    proc.wait()
                    
                    
                    self.output(sys.stdout, rel_path)
                    if self.srcsrv and vcs_root:
                        
                        self.SourceServerIndexing(file, guid, sourceFileStream, vcs_root)
                    
                    if self.copy_debug and arch_num == 0:
                        self.CopyDebug(file, debug_file, guid)
            except StopIteration:
                pass
            except e:
                self.output(sys.stderr, "Unexpected error: %s" % (str(e),))
                raise
            if result['status']:
                
                break
        return result




class Dumper_Win32(Dumper):
    fixedFilenameCaseCache = {}

    def ShouldProcess(self, file):
        """This function will allow processing of pdb files that have dll
        or exe files with the same base name next to them."""
        if not Dumper.ShouldProcess(self, file):
            return False
        if file.endswith(".pdb"):
            (path,ext) = os.path.splitext(file)
            if os.path.isfile(path + ".exe") or os.path.isfile(path + ".dll"):
                return True
        return False

    def FixFilenameCase(self, file):
        """Recent versions of Visual C++ put filenames into
        PDB files as all lowercase.  If the file exists
        on the local filesystem, fix it."""

        
        if file in self.fixedFilenameCaseCache:
            return self.fixedFilenameCaseCache[file]

        result = file

        (path, filename) = os.path.split(file)
        if os.path.isdir(path):
            lc_filename = filename.lower()
            for f in os.listdir(path):
                if f.lower() == lc_filename:
                    result = os.path.join(path, f)
                    break

        
        self.fixedFilenameCaseCache[file] = result
        return result

    def CopyDebug(self, file, debug_file, guid):
        rel_path = os.path.join(debug_file,
                                guid,
                                debug_file).replace("\\", "/")
        full_path = os.path.normpath(os.path.join(self.symbol_path,
                                                  rel_path))
        shutil.copyfile(file, full_path)
        
        compressed_file = os.path.splitext(full_path)[0] + ".pd_"
        
        success = subprocess.call(["makecab.exe", "/D", "CompressionType=LZX", "/D",
                                   "CompressionMemory=21",
                                   full_path, compressed_file],
                                  stdout=open("NUL:","w"), stderr=subprocess.STDOUT)
        if success == 0 and os.path.exists(compressed_file):
            os.unlink(full_path)
            self.output(sys.stdout, os.path.splitext(rel_path)[0] + ".pd_")
        else:
            self.output(sys.stdout, rel_path)
        
    def SourceServerIndexing(self, debug_file, guid, sourceFileStream, vcs_root):
        
        debug_file = os.path.abspath(debug_file)
        streamFilename = debug_file + ".stream"
        stream_output_path = os.path.abspath(streamFilename)
        
        result = SourceIndex(sourceFileStream, stream_output_path, vcs_root)
        if self.copy_debug:
            pdbstr_path = os.environ.get("PDBSTR_PATH")
            pdbstr = os.path.normpath(pdbstr_path)
            subprocess.call([pdbstr, "-w", "-p:" + os.path.basename(debug_file),
                             "-i:" + os.path.basename(streamFilename), "-s:srcsrv"],
                            cwd=os.path.dirname(stream_output_path))
            
            os.remove(stream_output_path)
        return result

class Dumper_Linux(Dumper):
    objcopy = os.environ['OBJCOPY'] if 'OBJCOPY' in os.environ else 'objcopy'
    def ShouldProcess(self, file):
        """This function will allow processing of files that are
        executable, or end with the .so extension, and additionally
        file(1) reports as being ELF files.  It expects to find the file
        command in PATH."""
        if not Dumper.ShouldProcess(self, file):
            return False
        if file.endswith(".so") or os.access(file, os.X_OK):
            return self.RunFileCommand(file).startswith("ELF")
        return False

    def CopyDebug(self, file, debug_file, guid):
        
        
        
        file_dbg = file + ".dbg"
        if subprocess.call([self.objcopy, '--only-keep-debug', file, file_dbg]) == 0 and \
           subprocess.call([self.objcopy, '--add-gnu-debuglink=%s' % file_dbg, file]) == 0:
            rel_path = os.path.join(debug_file,
                                    guid,
                                    debug_file + ".dbg")
            full_path = os.path.normpath(os.path.join(self.symbol_path,
                                                      rel_path))
            shutil.move(file_dbg, full_path)
            
            os.system("gzip %s" % full_path)
            self.output(sys.stdout, rel_path + ".gz")
        else:
            if os.path.isfile(file_dbg):
                os.unlink(file_dbg)

class Dumper_Solaris(Dumper):
    def RunFileCommand(self, file):
        """Utility function, returns the output of file(1)"""
        try:
            output = os.popen("file " + file).read()
            return output.split('\t')[1];
        except:
            return ""

    def ShouldProcess(self, file):
        """This function will allow processing of files that are
        executable, or end with the .so extension, and additionally
        file(1) reports as being ELF files.  It expects to find the file
        command in PATH."""
        if not Dumper.ShouldProcess(self, file):
            return False
        if file.endswith(".so") or os.access(file, os.X_OK):
            return self.RunFileCommand(file).startswith("ELF")
        return False

def StartProcessFilesWorkMac(dumper, file):
    """multiprocessing can't handle methods as Process targets, so we define
    a simple wrapper function around the work method."""
    return dumper.ProcessFilesWorkMac(file)

def AfterMac(status, dsymbundle):
    """Cleanup function to run on Macs after we process the file(s)."""
    
    shutil.rmtree(dsymbundle)

class Dumper_Mac(Dumper):
    def ShouldProcess(self, file):
        """This function will allow processing of files that are
        executable, or end with the .dylib extension, and additionally
        file(1) reports as being Mach-O files.  It expects to find the file
        command in PATH."""
        if not Dumper.ShouldProcess(self, file):
            return False
        if file.endswith(".dylib") or os.access(file, os.X_OK):
            return self.RunFileCommand(file).startswith("Mach-O")
        return False

    def ShouldSkipDir(self, dir):
        """We create .dSYM bundles on the fly, but if someone runs
        buildsymbols twice, we should skip any bundles we created
        previously, otherwise we'll recurse into them and try to 
        dump the inner bits again."""
        if dir.endswith(".dSYM"):
            return True
        return False

    def ProcessFiles(self, files, after=None, after_arg=None):
        
        
        self.output_pid(sys.stderr, "Submitting job for Mac pre-processing on file: %s" % (files[0]))
        self.SubmitJob(files[0], StartProcessFilesWorkMac, args=(self, files[0]), callback=self.ProcessFilesMacFinished)

    def ProcessFilesMacFinished(self, result):
        if result['status']:
            
            Dumper.ProcessFiles(self, result['files'], after=AfterMac, after_arg=result['files'][0])
        
        self.JobFinished(result['files'][-1])

    def ProcessFilesWorkMac(self, file):
        """dump_syms on Mac needs to be run on a dSYM bundle produced
        by dsymutil(1), so run dsymutil here and pass the bundle name
        down to the superclass method instead."""
        self.output_pid(sys.stderr, "Worker running Mac pre-processing on file: %s" % (file,))

        
        
        result = { 'status' : False, 'files' : None, 'file_key' : file }
        dsymbundle = file + ".dSYM"
        if os.path.exists(dsymbundle):
            shutil.rmtree(dsymbundle)
        
        subprocess.call(["dsymutil"] + [a.replace('-a ', '--arch=') for a in self.archs if a]
                        + [file],
                        stdout=open("/dev/null","w"))
        if not os.path.exists(dsymbundle):
            
            self.output_pid(sys.stderr, "No symbols found in file: %s" % (file,))
            result['status'] = False
            result['files'] = (file, )
            return result

        result['status'] = True
        result['files'] = (dsymbundle, file)
        return result

    def CopyDebug(self, file, debug_file, guid):
        """ProcessFiles has already produced a dSYM bundle, so we should just
        copy that to the destination directory. However, we'll package it
        into a .tar.bz2 because the debug symbols are pretty huge, and
        also because it's a bundle, so it's a directory. |file| here is the
        dSYM bundle, and |debug_file| is the original filename."""
        rel_path = os.path.join(debug_file,
                                guid,
                                os.path.basename(file) + ".tar.bz2")
        full_path = os.path.abspath(os.path.join(self.symbol_path,
                                                  rel_path))
        success = subprocess.call(["tar", "cjf", full_path, os.path.basename(file)],
                                  cwd=os.path.dirname(file),
                                  stdout=open("/dev/null","w"), stderr=subprocess.STDOUT)
        if success == 0 and os.path.exists(full_path):
            self.output(sys.stdout, rel_path)


def main():
    parser = OptionParser(usage="usage: %prog [options] <dump_syms binary> <symbol store path> <debug info files>")
    parser.add_option("-c", "--copy",
                      action="store_true", dest="copy_debug", default=False,
                      help="Copy debug info files into the same directory structure as symbol files")
    parser.add_option("-a", "--archs",
                      action="store", dest="archs",
                      help="Run dump_syms -a <arch> for each space separated cpu architecture in ARCHS (only on OS X)")
    parser.add_option("-s", "--srcdir",
                      action="append", dest="srcdir", default=[],
                      help="Use SRCDIR to determine relative paths to source files")
    parser.add_option("-v", "--vcs-info",
                      action="store_true", dest="vcsinfo",
                      help="Try to retrieve VCS info for each FILE listed in the output")
    parser.add_option("-i", "--source-index",
                      action="store_true", dest="srcsrv", default=False,
                      help="Add source index information to debug files, making them suitable for use in a source server.")
    parser.add_option("-x", "--exclude",
                      action="append", dest="exclude", default=[], metavar="PATTERN",
                      help="Skip processing files matching PATTERN.")
    parser.add_option("--repo-manifest",
                      action="store", dest="repo_manifest",
                      help="""Get source information from this XML manifest
produced by the `repo manifest -r` command.
""")
    parser.add_option("--install-manifest",
                      action="append", dest="install_manifests",
                      default=[],
                      help="""Use this install manifest to map filenames back
to canonical locations in the source repository. Specify
<install manifest filename>,<install destination> as a comma-separated pair.
""")
    (options, args) = parser.parse_args()

    
    if options.srcsrv:
        pdbstr = os.environ.get("PDBSTR_PATH")
        if not os.path.exists(pdbstr):
            print >> sys.stderr, "Invalid path to pdbstr.exe - please set/check PDBSTR_PATH.\n"
            sys.exit(1)

    if len(args) < 3:
        parser.error("not enough arguments")
        exit(1)

    try:
        manifests = validate_install_manifests(options.install_manifests)
    except (IOError, ValueError) as e:
        parser.error(str(e))
        exit(1)
    file_mapping = make_file_mapping(manifests)
    dumper = GetPlatformSpecificDumper(dump_syms=args[0],
                                       symbol_path=args[1],
                                       copy_debug=options.copy_debug,
                                       archs=options.archs,
                                       srcdirs=options.srcdir,
                                       vcsinfo=options.vcsinfo,
                                       srcsrv=options.srcsrv,
                                       exclude=options.exclude,
                                       repo_manifest=options.repo_manifest,
                                       file_mapping=file_mapping)
    for arg in args[2:]:
        dumper.Process(arg)
    dumper.Finish()


if __name__ == "__main__":
    
    
    Dumper.GlobalInit()

    main()
